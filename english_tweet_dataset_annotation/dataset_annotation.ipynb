{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic modelling on original english tweets dataset of size 16575 to generate 10 distinct topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraraies\n",
    "import pandas as pd\n",
    "import os, re, csv\n",
    "from csv import writer\n",
    "from csv import reader\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from operator import itemgetter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "processeddata = pd.read_csv('tweets[processed_english].csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "            result.append(token)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#map is used to tokenize\n",
    "processed_corpus = processeddata['text'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(26494 unique tokens: [u'minkasadhupyog', u'sivashiv', u'sation', u'disobeying', u'carryminati']...)\n"
     ]
    }
   ],
   "source": [
    "print dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=30, keep_n=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=200,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic:0 \n",
      "list_of_words: 0.438*\"aadharcard\" + 0.165*\"number\" + 0.110*\"aadhaar\" + 0.070*\"mobile\" + 0.057*\"details\" + 0.026*\"modi\" + 0.021*\"form\" + 0.018*\"says\" + 0.017*\"application\" + 0.016*\"birth\"\n",
      "\n",
      "\n",
      "topic:1 \n",
      "list_of_words: 0.216*\"india\" + 0.181*\"narendramodi\" + 0.095*\"pmoindia\" + 0.076*\"take\" + 0.074*\"office\" + 0.036*\"st\" + 0.035*\"work\" + 0.030*\"making\" + 0.029*\"used\" + 0.026*\"action\"\n",
      "\n",
      "\n",
      "topic:2 \n",
      "list_of_words: 0.144*\"address\" + 0.109*\"new\" + 0.102*\"proof\" + 0.066*\"photo\" + 0.065*\"give\" + 0.059*\"airtel\" + 0.056*\"asked\" + 0.046*\"information\" + 0.044*\"centre\" + 0.031*\"wrong\"\n",
      "\n",
      "\n",
      "topic:3 \n",
      "list_of_words: 0.096*\"video\" + 0.084*\"made\" + 0.065*\"good\" + 0.064*\"said\" + 0.063*\"plz\" + 0.049*\"news\" + 0.046*\"delhi\" + 0.044*\"country\" + 0.041*\"much\" + 0.041*\"thank\"\n",
      "\n",
      "\n",
      "topic:4 \n",
      "list_of_words: 0.085*\"linked\" + 0.079*\"kyc\" + 0.067*\"got\" + 0.064*\"paytm\" + 0.051*\"totally\" + 0.049*\"getting\" + 0.039*\"days\" + 0.038*\"again\" + 0.034*\"ask\" + 0.033*\"service\"\n",
      "\n",
      "\n",
      "topic:5 \n",
      "list_of_words: 0.122*\"need\" + 0.120*\"pan\" + 0.066*\"download\" + 0.053*\"data\" + 0.049*\"today\" + 0.046*\"asking\" + 0.043*\"easy\" + 0.031*\"branch\" + 0.028*\"adhar\" + 0.027*\"compulsory\"\n",
      "\n",
      "\n",
      "topic:6 \n",
      "list_of_words: 0.209*\"get\" + 0.203*\"bank\" + 0.193*\"account\" + 0.077*\"govt\" + 0.066*\"site\" + 0.041*\"copy\" + 0.039*\"updated\" + 0.035*\"open\" + 0.019*\"full\" + 0.018*\"private\"\n",
      "\n",
      "\n",
      "topic:7 \n",
      "list_of_words: 0.227*\"want\" + 0.202*\"government\" + 0.201*\"know\" + 0.060*\"share\" + 0.052*\"face\" + 0.046*\"someone\" + 0.042*\"pancard\" + 0.038*\"save\" + 0.027*\"start\" + 0.026*\"print\"\n",
      "\n",
      "\n",
      "topic:8 \n",
      "list_of_words: 0.069*\"linking\" + 0.060*\"youtube\" + 0.050*\"day\" + 0.044*\"old\" + 0.042*\"apply\" + 0.036*\"required\" + 0.032*\"back\" + 0.030*\"right\" + 0.030*\"till\" + 0.028*\"pm\"\n",
      "\n",
      "\n",
      "topic:9 \n",
      "list_of_words: 0.105*\"sim\" + 0.070*\"following\" + 0.062*\"use\" + 0.055*\"pls\" + 0.045*\"privacy\" + 0.045*\"jio\" + 0.042*\"document\" + 0.039*\"think\" + 0.039*\"told\" + 0.038*\"due\"\n",
      "\n",
      "\n",
      "topic:10 \n",
      "list_of_words: 0.340*\"uidai\" + 0.103*\"online\" + 0.085*\"name\" + 0.075*\"way\" + 0.053*\"change\" + 0.047*\"ceo\" + 0.032*\"phone\" + 0.023*\"vote\" + 0.022*\"let\" + 0.017*\"numbers\"\n",
      "\n",
      "\n",
      "topic:11 \n",
      "list_of_words: 0.223*\"like\" + 0.140*\"other\" + 0.116*\"issue\" + 0.093*\"valid\" + 0.085*\"make\" + 0.084*\"very\" + 0.032*\"reason\" + 0.029*\"taking\" + 0.028*\"hello\" + 0.024*\"party\"\n",
      "\n",
      "\n",
      "topic:12 \n",
      "list_of_words: 0.077*\"last\" + 0.070*\"same\" + 0.059*\"certificate\" + 0.054*\"since\" + 0.051*\"accounts\" + 0.048*\"year\" + 0.046*\"given\" + 0.044*\"police\" + 0.041*\"problem\" + 0.038*\"say\"\n",
      "\n",
      "\n",
      "topic:13 \n",
      "list_of_words: 0.194*\"please\" + 0.134*\"help\" + 0.104*\"update\" + 0.082*\"time\" + 0.049*\"request\" + 0.042*\"process\" + 0.039*\"system\" + 0.035*\"saying\" + 0.033*\"kindly\" + 0.033*\"center\"\n",
      "\n",
      "\n",
      "topic:14 \n",
      "list_of_words: 0.334*\"link\" + 0.221*\"about\" + 0.105*\"call\" + 0.075*\"care\" + 0.047*\"remove\" + 0.044*\"supremecourt\" + 0.032*\"voters\" + 0.029*\"regarding\" + 0.026*\"election\" + 0.020*\"nd\"\n",
      "\n",
      "\n",
      "topic:15 \n",
      "list_of_words: 0.099*\"passport\" + 0.090*\"opening\" + 0.075*\"indian\" + 0.066*\"documents\" + 0.061*\"banks\" + 0.061*\"because\" + 0.056*\"person\" + 0.053*\"verification\" + 0.051*\"months\" + 0.047*\"fake\"\n",
      "\n",
      "\n",
      "topic:16 \n",
      "list_of_words: 0.269*\"people\" + 0.097*\"bjp\" + 0.090*\"done\" + 0.087*\"money\" + 0.086*\"sc\" + 0.058*\"using\" + 0.050*\"years\" + 0.038*\"times\" + 0.036*\"wallet\" + 0.030*\"poor\"\n",
      "\n",
      "\n",
      "topic:17 \n",
      "list_of_words: 0.198*\"come\" + 0.159*\"date\" + 0.125*\"original\" + 0.111*\"digital\" + 0.060*\"exam\" + 0.052*\"near\" + 0.046*\"students\" + 0.038*\"par\" + 0.036*\"airport\" + 0.030*\"nagar\"\n",
      "\n",
      "\n",
      "topic:18 \n",
      "list_of_words: 0.319*\"twitter\" + 0.185*\"pic\" + 0.084*\"court\" + 0.069*\"supreme\" + 0.043*\"go\" + 0.033*\"customer\" + 0.029*\"rs\" + 0.020*\"submitted\" + 0.020*\"connection\" + 0.019*\"necessary\"\n",
      "\n",
      "\n",
      "topic:19 \n",
      "list_of_words: 0.168*\"id\" + 0.151*\"mandatory\" + 0.096*\"status\" + 0.091*\"voter\" + 0.047*\"every\" + 0.040*\"identity\" + 0.039*\"check\" + 0.031*\"waiting\" + 0.030*\"well\" + 0.029*\"scheme\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, words in lda_model.print_topics():\n",
    "    print('topic:{} \\nlist_of_words: {}'.format( i, words))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.13312648),\n",
       " (1, 0.03956868),\n",
       " (2, 0.040368736),\n",
       " (3, 0.032686394),\n",
       " (4, 0.086468145),\n",
       " (5, 0.06586392),\n",
       " (6, 0.054633163),\n",
       " (7, 0.018266905),\n",
       " (8, 0.05149425),\n",
       " (9, 0.07442124),\n",
       " (10, 0.053086374),\n",
       " (11, 0.024997832),\n",
       " (12, 0.0351393),\n",
       " (13, 0.046823844),\n",
       " (14, 0.02276962),\n",
       " (15, 0.055411257),\n",
       " (16, 0.02569849),\n",
       " (17, 0.010889462),\n",
       " (18, 0.08371193),\n",
       " (19, 0.044573966)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#analysing new sentence(tweet) by % of belonging from each topic\n",
    "lda_model[dictionary.doc2bow(tokenize(\"mobile sim number linked with adhaar\"))][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset annotation based on generated 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading and reading dataset\n",
    "#tweets[processed_english] has 16575 tweets\n",
    "#these tweets are detected as english using BLD model\n",
    "english_tweets = open(\"tweets[processed_english].csv\", \"r\")\n",
    "english_tweets = english_tweets.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning of tweets\n",
    "#elimination of word whose length is less than 3\n",
    "#elimination of tweets which has word length less than 5 and more than 15\n",
    "#creation of new csv file tweets[more_processed_english].csv\n",
    "for line in english_tweets.split('\\n'):\n",
    "    if line:\n",
    "        sentence=[]\n",
    "        for word in line.split(' '):\n",
    "            if len(word)>2:\n",
    "                sentence.append(word)\n",
    "        if len(sentence)>=5 and len(sentence)<=15:\n",
    "            with open('tweets[processed_english_for_labeling].csv', 'a') as csvFile:\n",
    "                writer = csv.writer(csvFile)\n",
    "                writer.writerow(sentence)\n",
    "            csvFile.close()\n",
    "with open('tweets[processed_english_for_labeling].csv') as f:\n",
    "    f=f.read()\n",
    "    re1 = re.compile(',')\n",
    "    file = re1.sub(' ', f)\n",
    "with open('tweets[processed_english_for_labeling].csv','w') as F:\n",
    "    F.write(file)\n",
    "    F.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic no: 0  value: 0.1331265\n"
     ]
    }
   ],
   "source": [
    "#visualizing and labelling topic no to a new tweet based on max % of belonging from that topic\n",
    "test_tweet=\"mobile sim number linked with adhaar\"\n",
    "lda_model[dictionary.doc2bow(tokenize(test_tweet))][0]\n",
    "#clearly 6 is topic no as more than 20 % of belonging is from that topic\n",
    "topic_no=max(lda_model[dictionary.doc2bow(tokenize(test_tweet))][0], key = itemgetter(1))[0]\n",
    "value=max(lda_model[dictionary.doc2bow(tokenize(test_tweet))][0], key = itemgetter(1))[1]\n",
    "print \"topic no:\", topic_no,\" value:\", value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opening tweets[more_processed_english].csv in read mode and semi_labeled.csv in write mode\n",
    "# Only those tweets which has atleast 20% of belonging from the topics and topic no corresponding to maximum value of belonging will be assigned a label\n",
    "from csv import writer\n",
    "from csv import reader\n",
    "with open('tweets[processed_english_for_labeling].csv', 'r') as read_obj, \\\n",
    "        open('tweets[labeled].csv', 'w') as write_obj:\n",
    "    csv_reader = reader(read_obj)\n",
    "    csv_writer = writer(write_obj)\n",
    "    for row in csv_reader:\n",
    "        #if (max(lda_model[dictionary.doc2bow(tokenize(row[0]))][0], key = itemgetter(1))[1])>.2:\n",
    "        row.append(max(lda_model[dictionary.doc2bow(tokenize(row[0]))][0], key = itemgetter(1))[0])\n",
    "        csv_writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
